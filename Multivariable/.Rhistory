#Tabla con datos de las n=2469 observaciones
Tabla1<-Datos %>%
summarize("n"=n(),
"Pob. Analfabeta" = round(mean(ANALF),1),
"Pob. 15 sin educación básica" =round(mean(SBASC),1),
"Pob. en viv. sin drenaje"=round(mean(OVSDE),1),
"Pob. en viv. sin energia"= round(mean(OVSEE),1),
"Pob. en viv. sin agua entubada"=round(mean(OVSAE),1),
"Pob. en viv. con piso tierra" = round(mean(OVPT),1),
"Pob. en viv. con hacinamiento" = round(mean(VHAC),1),
"Pob. en menos 5mil hab."= round(mean(PL.5000),1),
"Pob. con ingresos 3 sal. min." = round(mean(PO2SM),1),
"Indice1" = round(mean(Indice1),1),
"Indice2"= round(mean(Indice2),1),
"Indice"= round(mean(Indice),1))
#Tabla por cada cluster
Tabla2<-Datos %>%
group_by(ClusterKM) %>%
summarize("n"=n(),
"Pob. Analfabeta" = round(mean(ANALF),1),
"Pob. 15 sin educación básica" =round(mean(SBASC),1),
"Pob. en viv. sin drenaje"=round(mean(OVSDE),1),
"Pob. en viv. sin energia"= round(mean(OVSEE),1),
"Pob. en viv. sin agua entubada"=round(mean(OVSAE),1),
"Pob. en viv. con piso tierra" = round(mean(OVPT),1),
"Pob. en viv. con hacinamiento" = round(mean(VHAC),1),
"Pob. en menos 5mil hab."= round(mean(PL.5000),1),
"Pob. con ingresos 3 sal. min." = round(mean(PO2SM),1),
"Indice1" = round(mean(Indice1),1),
"Indice2"= round(mean(Indice2),1),
"Indice"= round(mean(Indice),1))
#Tabla agregada
Tabla <-bind_rows(Tabla2, Tabla1)
t(Tabla)
library(readxl)
Datos<-read_excel("DatosF.xlsx", sheet="datos")
D1 <- DatosF[c(6:35)]
library(readxl)
Datos<-read_excel("DatosF.xlsx", sheet="datos")
D1 <- Datos[c(6:35)]
View(D1)
View(D1)
colSums(is.na(D1))
library(psych)
describe(D1)
library(car)
scatterplotMatrix(D1,smooth=FALSE)
library(psych)
corPlot(D1,cex = .6,stars = TRUE, show.legend=FALSE)
#Estimar la matriz de correlaciones
R<-cor(D1)
#Calcular el determinante de la matriz
det(R)
library(psych)
cortest.bartlett(R,n=38)
library(psych)
KMO(R)
D1<-D1[,-27]
library(psych)
R<-cor(D1)
KMO(R)
D1<-D1[,-25]     #Para quitar V6
library(psych)
R<-cor(D1)
KMO(R)
D1<-D1[,-26]     #Para quitar V6
library(psych)
R<-cor(D1)
KMO(R)
D1<-D1[,-24]     #Para quitar V6
library(psych)
R<-cor(D1)
KMO(R)
D1<-D1[,-3]     #Para quitar V6
library(psych)
R<-cor(D1)
KMO(R)
D1<-D1[,-1]     #Para quitar V6
library(psych)
R<-cor(D1)
KMO(R)
D1<-D1[,-5]     #Para quitar V6
library(psych)
R<-cor(D1)
KMO(R)
Dz <- data.frame(scale(D1)) #Estandarizar las variables para que estén en la misma escala
library(psych)
modelo1<-principal(Dz,nfactors = 23, rotate= "none")
modelo1
library(psych)
R<-cor(D1)      #Matriz de correlación
scree(R)         #Gráfico de sedimentación
library(psych)
modelo2<-principal(Dz,nfactors = 5, rotate='none')
modelo2
library(psych)
modelo2<-principal(Dz,nfactors = 2, rotate='none')
modelo2
library(psych)
modelo2<-principal(Dz,nfactors = 2, rotate="varimax")
modelo2
print(modelo2$loadings,cut=0.5, sort=TRUE)
#Del modelo 2 saca las cargas y desaparece las que tenga menos de o.5 (Propiedad 1 =0.5)
print(modelo2$loadings,cut=0.4, sort=TRUE) #(Prioridad 2 = 0.4)
library(psych)
fa.diagram(modelo2)
load <- modelo1$loadings[,1:2]      #obtiene las cargas de las variables
plot(load,type="n")                 # Dibujar el área de visualización
text(load,labels=names(Dz),cex=.7)  # Añadir las variables
library(psych)
modelo2<-principal(Dz,nfactors = 2, rotate="varimax",
scores=TRUE,method="regression") #con el método que queramos calcular los puntajes
Dz$PC1<-modelo2$scores[,1]
Dz$PC2<-modelo2$scores[,2] #salen en la base de datos en otra columna
modelo2$weights #
View(Dz)
View(Dz)
library(psych)
alpha(Dz[,c(4,7,8,10,11,12,13,14,15,16,17,19,20,21,22,23)],check.keys=TRUE) #variables a utilizar
library(psych)
alpha(Dz[,c(1,2,3,5,6,9,18,)],check.keys=TRUE)
library(psych)
alpha(Dz[,c(1,2,3,5,6,9,18)],check.keys=TRUE)
#Componente 1
Dz$In_marginacion<-with(Dz,max(In_marginacion)-In_marginacion)
Dz$razon_ingreso<-with(Dz,max(razon_ingreso)-razon_ingreso)
Dz$incidencia_delic<-with(Dz,max(incidencia_delic)-incidencia_delic)
Dz$personal_seg<-with(Dz,max(personal_seg)-personal_seg)
#Una vez que se cambio la escala se tiene que volver a estimar el modelo y los componentes.
library(psych)
modelo3<-principal(Dz,nfactors = 2, rotate="varimax",
scores=TRUE,method="regression") #con el método que queramos calcular los puntajes
Dz$PC1<-modelo3$scores[,1]
Dz$PC2<-modelo3$scores[,2]
modelo3
Dz$Indice1<-with(Dz,100*(PC1-min(PC1))/(max(PC1)-min(PC1))) #Componente 1
Dz$Indice2<-with(Dz,100*(PC2-min(PC2))/(max(PC2)-min(PC2))) #Componente 2
modelo3
#Ponderadores obtenidos del modelo estimado, renglon "Proportion Var".
w1=0.43/(0.43+0.28)
w2=0.28/(0.28+0.43)
#Indice global de Marginación de la base de datos
Dz$Indice<-with(Dz,w1*Indice1+w2*Indice2) #ponderando los dos índices, nuestro final se llama columna Índice
View(Dz)
View(Dz)
library(writexl)
write_xlsx(Datos,"DatosCluster.xlsx")
library(writexl)
write_xlsx(Dz,"DatosCluster.xlsx")
library(readxl)
Datos<-read_excel("DatosF.xlsx", sheet="datos")
D1 <- Datos[c(6:35)]
colSums(is.na(D1))
library(psych)
describe(D1)
library(car)
scatterplotMatrix(D1,smooth=FALSE)
install.packages(GGally)
install.packages("GGally")
library(GGally)
ggpairs(D1, title="correlogram with ggpairs()")
library(psych)
corPlot(D1,cex = .6,stars = TRUE, show.legend=FALSE)
#Estimar la matriz de correlaciones
R<-cor(D1)
#Calcular el determinante de la matriz
det(R)
library(psych)
cortest.bartlett(R,n=38)
library(psych)
KMO(R)
D1<-D1[,-27]
library(psych)
R<-cor(D1)
KMO(R)
D1<-D1[,-25]     #Para quitar V6
library(psych)
R<-cor(D1)
KMO(R)
D1<-D1[,-26]     #Para quitar V6
library(psych)
R<-cor(D1)
KMO(R)
D1<-D1[,-24]     #Para quitar V6
library(psych)
R<-cor(D1)
KMO(R)
D1<-D1[,-3]     #Para quitar V6
library(psych)
R<-cor(D1)
KMO(R)
D1<-D1[,-1]     #Para quitar V6
library(psych)
R<-cor(D1)
KMO(R)
D1<-D1[,-5]     #Para quitar V6
library(psych)
R<-cor(D1)
KMO(R)
Dz <- data.frame(scale(D1)) #Estandarizar las variables para que estén en la misma escala
library(psych)
modelo1<-principal(Dz,nfactors = 23, rotate= "none")
modelo1
library(psych)
R<-cor(D1)      #Matriz de correlación
scree(R)         #Gráfico de sedimentación
library(psych)
modelo2<-principal(Dz,nfactors = 5, rotate='none')
modelo2
library(psych)
modelo2<-principal(Dz,nfactors = 2, rotate='none')
modelo2
library(psych)
modelo2<-principal(Dz,nfactors = 2, rotate="varimax")
modelo2
library(psych)
modelo2<-principal(Dz,nfactors = 2, rotate="varimax")
modelo2
library(psych)
modelo2<-principal(Dz,nfactors = 2, rotate="varimax")
modelo2
print(modelo2$loadings,cut=0.5, sort=TRUE)
#Del modelo 2 saca las cargas y desaparece las que tenga menos de o.5 (Propiedad 1 =0.5)
print(modelo2$loadings,cut=0.4, sort=TRUE) #(Prioridad 2 = 0.4)
library(psych)
fa.diagram(modelo2)
load <- modelo1$loadings[,1:2]      #obtiene las cargas de las variables
plot(load,type="n")                 # Dibujar el área de visualización
text(load,labels=names(Dz),cex=.7)  # Añadir las variables
library(psych)
modelo2<-principal(Dz,nfactors = 2, rotate="varimax",
scores=TRUE,method="regression") #con el método que queramos calcular los puntajes
Dz$PC1<-modelo2$scores[,1]
Dz$PC2<-modelo2$scores[,2] #salen en la base de datos en otra columna
modelo2$weights #
library(psych)
alpha(Dz[,c(4,7,8,10,11,12,13,14,15,16,17,19,20,21,22,23)],check.keys=TRUE) #variables a utilizar
library(psych)
alpha(Dz[,c(4,7,8,10,11,12,13,14,15,16,17,19,20,21,22,23)], check.keys=TRUE) #variables a utilizar
library(psych)
alpha(Dz[,c(1,2,3,5,6,9,18)],check.keys=TRUE)
#Abrir excel
library(readxl)
Datos <- read_excel("DatosEF.xlsx", sheet="datos")
setwd("C:\Users\veron\OneDrive\Multivariante\Evidencia\Evidencia")
setwd("C:/Users/veron/OneDrive/Multivariante/Evidencia/Evidencia")
#Abrir excel
library(readxl)
Datos <- read_excel("DatosEF.xlsx", sheet="datos")
#Abrir excel
library(readxl)
Datos <- read_excel("DatosEF.xlsx", sheet="datos")
library(readxl)
DatosEF_xlsx <- read_excel("DatosEF.xlsx.xlsx")
View(DatosEF_xlsx)
#Abrir excel
library(readxl)
Datos <- read_excel("DatosEF.xlsx.xlsx")
#Abrir excel
library(readxl)
Datos <- read_excel("DatosEF.xlsx")
setwd("C:/Users/veron/OneDrive/Multivariante/Evidencia/Evidencia")
#Abrir excel
library(readxl)
Datos <- read_excel("DatosEF.xlsx")
#Abrir excel
library(readxl)
Datos <- read_excel("DatosEF.xlsx")
#Abrir excel
library(readxl)
Datos <- read_excel("DatosEF.xlsx")
D1<-Datos[,c(6:35)]
View(D1)
View(D1)
View(Datos)
View(Datos)
#Abrir excel
library(readxl)
Datos <- read_excel("DatosEF.xlsx")
View(Datos)
View(Datos)
D1<-Datos[,c(6:38)]
View(D1)
View(D1)
colSums(is.na(D1))
D1z <- scale(D1)
d <- dist(D1z, method = "euclidean")
library(factoextra)
HC <- hcut(d, hc_method = "ward.D")
HC
fviz_dend(HC)
HC <- hcut(d, k=4, hc_method = "ward.D")
fviz_dend(HC, show_labels = FALSE, rect = TRUE)
Datos$ClusterHC<-HC$cluster
fviz_cluster(HC,
data=Datos[,c("PNA","FTE")],
stand = FALSE,
geom="point",
mean.point.size=5)+
scale_y_continuous(breaks = seq(0,1.2,0.1))+
scale_x_continuous(breaks = seq(70,100,5))
fviz_cluster(HC,
data=Datos[,c(6:35)],
stand = FALSE,
geom="point",
mean.point.size=5)+
scale_y_continuous(breaks = seq(0,1.2,0.1))+
scale_x_continuous(breaks = seq(70,100,5))
fviz_cluster(HC,
data=Datos[,c(6:35)],
geom="point",
stand = FALSE,
mean.point.size=5)+
scale_y_continuous(breaks = seq(1,7,1))+
scale_x_continuous(breaks = seq(70,100,5))
#Abrir excel
library(readxl)
Datos <- read_excel("DatosEF.xlsx")
View(Datos)
View(Datos)
D1<-Datos[,c(6:28)]
View(D1)
View(D1)
colSums(is.na(D1))
D1z <- scale(D1)
d <- dist(D1z, method = "euclidean")
library(factoextra)
HC <- hcut(d, hc_method = "ward.D")
HC
fviz_dend(HC)
HC <- hcut(d, k=4, hc_method = "ward.D")
HC <- hcut(d, k=4, hc_method = "ward.D")
fviz_dend(HC, show_labels = FALSE, rect = TRUE)
Datos$ClusterHC<-HC$cluster
fviz_cluster(HC,
data=Datos[,c(6:28)],
stand = FALSE,
geom="point",
mean.point.size=5)+
scale_y_continuous(breaks = seq(0,1.2,0.1))+
scale_x_continuous(breaks = seq(70,100,5))
#Abrir excel
library(readxl)
Datos <- read_excel("DatosEF.xlsx")
View(Datos)
View(Datos)
D1<-Datos[,c(6:28)]
View(D1)
View(D1)
colSums(is.na(D1))
D1z <- scale(D1)
d <- dist(D1z, method = "euclidean")
library(factoextra)
HC <- hcut(d, hc_method = "ward.D")
HC
fviz_dend(HC)
HC <- hcut(d, k=4, hc_method = "ward.D")
fviz_dend(HC, show_labels = FALSE, rect = TRUE)
Datos$ClusterHC<-HC$cluster
fviz_cluster(HC,
data=Datos[,c(37:38)],
stand = FALSE,
geom="point",
mean.point.size=5)+
scale_y_continuous(breaks = seq(0,1.2,0.1))+
scale_x_continuous(breaks = seq(70,100,5))
library(readxl)
Datos <-read_excel("DatosEF.xlsx",sheet="datos")
D1<-Datos[,c("Indice1","Indice2")]
colSums(is.na(D1))
D1z<-scale(D1)
d<-dist(D1z, method="euclidean")
View(Datos)
View(Datos)
library(factoextra)
HC <- hcut(d, hc_method = "ward.D")
HC
fviz_dend(HC)
library(factoextra)
fviz_nbclust(D1z, kmeans, method ="wss")
HC <- hcut(d, k=4, hc_method = "ward.D")
fviz_dend(HC, show_labels = FALSE, rect = TRUE)
Datos$ClusterHC<-HC$cluster
library("dplyr")
#Tabla con datos de las n=38 observaciones
Tabla1<-Datos %>%
summarize("n"=n(),
"% viv. con agua" = round(mean(con_agua),1),
"% viv. con TIC" = round(mean(con_tic),1),
"% viv. sin carencia de viv." = round(mean(sincar_vivienda),1),
"% pob. no vulnerable por carencias" = round(mean(novul_carencias),1),
"% pob. no en probreza" = round(mean(pob_nopobreza),1),
"% pob. con ingreso mayor a línea de bienestar" = round(mean(sin_ingresoinf),1),
"PIB per cápita" = round(mean(PIB_PC),1),
"IDH" = round(mean(IDH),1),
"Longevidad" = round(mean(logevidad),1),
"Marginación" = round(mean(In_marginacion),1),
"% pob. sin rezago edu." = round(mean(sinrezago_edu),1),
"% pob. letrada" = round(mean(no_analf),1),
"% pob. con edu. básica" = round(mean(con_eduba),1),
"Grado de escolaridad" = round(mean(prom_edu),1),
"Sucursales bancarias" = round(mean(total_sucursales),1),
"# tarjetas de crédito" = round(mean(num_ctas),1),
"# contratos de banca móvil" = round(mean(banca_movil),1),
"% viv. sin piso de tierra" = round(mean(sinpiso_tierra),1),
"Gini invertido" = round(mean(in_gini),1),
"Razón de ingreso" = round(mean(razon_ingreso),1),
"% pob. sin carencias seguridad social" = round(mean(sincar_salud),1),
"Tasa de Incidencia Delictiva Municipal" = round(mean(incidencia_delic),1),
"Personal destinado a funciones de seguridad" = round(mean(personal_seg),1),
"Indice1" = round(mean(Indice1),1),
"Indice2"= round(mean(Indice2),1),
"Indice"= round(mean(Indice),1))
#Tabla por cada cluster
Tabla2<-Datos %>%
group_by(ClusterHC) %>%
summarize("n"=n(),
"% viv. con agua" = round(mean(con_agua),1),
"% viv. con TIC" = round(mean(con_tic),1),
"% viv. sin carencia de viv." = round(mean(sincar_vivienda),1),
"% pob. no vulnerable por carencias" = round(mean(novul_carencias),1),
"% pob. no en probreza" = round(mean(pob_nopobreza),1),
"% pob. con ingreso mayor a línea de bienestar" = round(mean(sin_ingresoinf),1),
"PIB per cápita" = round(mean(PIB_PC),1),
"IDH" = round(mean(IDH),1),
"Longevidad" = round(mean(logevidad),1),
"Marginación" = round(mean(In_marginacion),1),
"% pob. sin rezago edu." = round(mean(sinrezago_edu),1),
"% pob. letrada" = round(mean(no_analf),1),
"% pob. con edu. básica" = round(mean(con_eduba),1),
"Grado de escolaridad" = round(mean(prom_edu),1),
"Sucursales bancarias" = round(mean(total_sucursales),1),
"# tarjetas de crédito" = round(mean(num_ctas),1),
"# contratos de banca móvil" = round(mean(banca_movil),1),
"% viv. sin piso de tierra" = round(mean(sinpiso_tierra),1),
"Gini invertido" = round(mean(in_gini),1),
"Razón de ingreso" = round(mean(razon_ingreso),1),
"% pob. sin carencias seguridad social" = round(mean(sincar_salud),1),
"Tasa de Incidencia Delictiva Municipal" = round(mean(incidencia_delic),1),
"Personal destinado a funciones de seguridad" = round(mean(personal_seg),1),
"Indice1" = round(mean(Indice1),1),
"Indice2"= round(mean(Indice2),1),
"Indice"= round(mean(Indice),1))
#Tabla agregada
Tabla <-bind_rows(Tabla2, Tabla1)
t(Tabla)
library(writexl)
write_xlsx(Datos,"DatosFINALCLUSTERS")
library(writexl)
write_xlsx(Datos,"DatosFINALCLUSTERS.xlsx")
fviz_cluster(HC,
data= Datos[,c("Indice1","Indice2")]
geom="point",
View(Datos)
View(Datos)
fviz_cluster(HC,
data= Datos[,c(37:38)]
geom="point",
fviz_cluster(HC,
data= Datos[,c("con_agua", "con_tic")]
geom="point",
fviz_cluster(HC,
data= Datos[c("con_agua", "con_tic")],
geom="point",
stand = FALSE,
mean.point.size=5)+
scale_y_continuous(breaks = seq(1,7,1))+
scale_x_continuous(breaks = seq(70,100,5))
fviz_cluster(HC,
data= Datos[c("Indice1", "Indice2")],
geom="point",
stand = FALSE,
mean.point.size=5)+
scale_y_continuous(breaks = seq(1,7,1))+
scale_x_continuous(breaks = seq(70,100,5))
fviz_cluster(HC,
data= Datos[c("PIB_PC", "IDH")],
geom="point",
stand = FALSE,
mean.point.size=5)+
scale_y_continuous(breaks = seq(1,7,1))+
scale_x_continuous(breaks = seq(70,100,5))
library(readxl)
Datos <-read_excel("DatosEF.xlsx",sheet="datos")
D1<-Datos[,c("Indice1","Indice2")]
colSums(is.na(D1))
D1z<-scale(D1)
d<-dist(D1z, method="euclidean")
library(factoextra)
HC <- hcut(d, hc_method = "ward.D")
HC
fviz_dend(HC)
library(factoextra)
fviz_nbclust(D1z, kmeans, method ="wss")
HC <- hcut(d, k=4, hc_method = "ward.D")
fviz_dend(HC, show_labels = FALSE, rect = TRUE)
fviz_cluster(HC,
data= Datos[c("PIB_PC", "Ind_marginacion")],
geom="point",
stand = FALSE,
mean.point.size=5)+
scale_y_continuous(breaks = seq(1,7,1))+
scale_x_continuous(breaks = seq(70,100,5))
fviz_cluster(HC,
data= Datos[c("PIB_PC", "In_marginacion")],
geom="point",
stand = FALSE,
mean.point.size=5)+
scale_y_continuous(breaks = seq(1,7,1))+
scale_x_continuous(breaks = seq(70,100,5))
View(D1)
View(D1)
